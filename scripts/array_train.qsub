#!/bin/bash -l

# Set SCC project
#$ -P tianlabdl

# Specify hard time limit for the job. 
#   The job will be aborted if it runs longer than this time.
#   The default time is 12 hours
#$ -l h_rt=48:00:00

# folder where the output of the qsub job is saved
#$ -o /projectnb/tianlabdl/jalido/.log/.qlog

# Combine output and error files into a single file
#$ -j y

# Keep track of information related to the current job
echo "=========================================================="
echo "Start date : $(date)"
echo "Job name : $JOB_NAME"
echo "Job ID : $JOB_ID  $SGE_TASK_ID"
echo "=========================================================="

# Specify number of cores 16: a whole node with at least 128gb ram
#$-pe omp 4
#$-t 15

#Specify the number of GPUs (1 is recommended!)
#$-l gpus=1
#$-l gpu_c=7.0
#$ -l gpu_memory=16G

# module load python3/3.8.10
# source /projectnb2/tianlabdl/venvs/syn_vasc_env/bin/activate
module load miniconda
conda activate /projectnb/tianlabdl/jalido/sbrnet_proj/.direnv
python /projectnb/tianlabdl/jalido/sbrnet_proj/sbrnet_core/sbrnet/main.py \
  --model_dir "/projectnb/tianlabdl/jalido/sbrnet_proj/trained_models/" \
  --batch_size 20 \
  --num_resblocks 18 \
  --view_ind $SGE_TASK_ID \
  --point_loss_weight 100000 \
  --qlo_weight 1 \
  --qhi_weight 1 \
  --learning_rate 0.00001 \
  --lr_scheduler "cosine_annealing" \
  --last_layer "quantile_heads" \
  --cosine_annealing_T_max 30 \
  --num_head_layers 2 \
  --weight_decay 0 \
  --optimizer "adam" \
  --q_lo 0.05 \
  --q_hi 0.95 \
  --output_activation "sigmoid" \
  --backbone "resnet" \
  --patch_size 128 \
  --dataset_pq "/ad/eng/research/eng_research_cisl/jalido/sbrnet/data/training_data/UQ/$SGE_TASK_ID/metadata.pq" \
  # --dataset_pq "/ad/eng/research/eng_research_cisl/jalido/sbrnet/data/training_data/UQ/vasc/$SGE_TASK_ID/metadata.pq" \
  # 
  # --criterion_name "mae"